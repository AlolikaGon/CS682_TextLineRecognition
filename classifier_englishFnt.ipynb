{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classifier_englishFnt.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqAajziKU2Wu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8b0e03b-9037-4c63-d573-b864719b2689"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUfyNQxaVpUx"
      },
      "source": [
        "# from google.colab import files\n",
        "# files.os.listdir('/gdrive/My Drive/')\n",
        "import tarfile\n",
        "tf = tarfile.open('/gdrive/My Drive/datasets/EnglishFnt.tgz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXcbRIP5V8_V"
      },
      "source": [
        "tf.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmCqWoPZWN4l"
      },
      "source": [
        "import numpy as np\n",
        "from skimage import io, transform\n",
        "from skimage.color import rgb2gray\n",
        "from skimage.transform import resize\n",
        "from random import shuffle\n",
        "\n",
        "X_train=[]\n",
        "X_test=[]\n",
        "Y_train=[]\n",
        "Y_test=[]\n",
        "#read images\n",
        "for i in range(1,63):\n",
        "  y = np.zeros(62)\n",
        "  y[i-1] = 1\n",
        "  s=\"%03d\"%i\n",
        "  directory=\"./English/Fnt/Sample\"+s\n",
        "  index=np.arange(1,1017)\n",
        "  shuffle(index)\n",
        "  for j in range(0,1016):\n",
        "    s2=\"%05d\"%index[j]\n",
        "    img_name=directory+\"/img\"+s+\"-\"+s2+\".png\"\n",
        "    img=io.imread(img_name)\n",
        "    img = rgb2gray(img)\n",
        "    img = resize(img, (15,19))\n",
        "    if j<int(0.9*1016): #split into training, val, and test\n",
        "      X_train.append(img)\n",
        "      Y_train.append(y)\n",
        "    else:\n",
        "      X_test.append(img)\n",
        "      Y_test.append(y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5rj--pMg_te",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd6c8e8d-2f19-4177-ec19-bc5b066100d7"
      },
      "source": [
        "X_train=np.asarray(X_train)/255\n",
        "X_train = X_train.reshape((X_train.shape[0], 15, 19, 1))\n",
        "X_test=np.asarray(X_test)/255\n",
        "X_test = X_test.reshape((X_test.shape[0], 15, 19, 1))\n",
        "Y_train=np.asarray(Y_train)\n",
        "Y_test=np.asarray(Y_test)\n",
        "\n",
        "print(\"Training dataset \", X_train.shape, Y_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training dataset  (56668, 15, 19, 1) (56668, 62)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs4kl99v1Wl9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3fffe00-14de-492b-e93a-98f6f3d996a3"
      },
      "source": [
        "len(X_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "56668"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnWetUbeh9nX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "843afd68-8ca0-44a7-c503-4836b93cd965"
      },
      "source": [
        "from keras.models import Model, Sequential\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers import Input, Dense, Activation, ZeroPadding2D, Flatten, Conv2D, MaxPooling2D, Dropout\n",
        "from keras import regularizers\n",
        "import keras\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras import regularizers\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(Conv2D(8, kernel_size=(3, 3), padding = 'valid', strides = (1,1), input_shape=(15, 19, 1), kernel_regularizer =regularizers.l2(0.001) ))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Activation('relu'))\n",
        "# model.add(Conv2D(8, kernel_size=(3, 3), padding = 'same', strides = (1,1), kernel_regularizer=regularizers.l2(0.01)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Activation('relu'))\n",
        "# model.add(Conv2D(16, kernel_size=(3, 3), padding = 'same', strides = (2,2), kernel_regularizer=regularizers.l2(0.01)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Activation('relu'))\n",
        "# model.add(Conv2D(16, kernel_size=(3, 3), padding = 'same', strides = (1,1), kernel_regularizer=regularizers.l2(0.01)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Activation('relu'))\n",
        "# model.add(Conv2D(16, kernel_size=(3, 3), padding = 'same', strides = (1,1), kernel_regularizer=regularizers.l2(0.01)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Activation('relu'))\n",
        "# model.add(Conv2D(32, kernel_size=(3, 3), padding = 'same', strides = (2,2), kernel_regularizer=regularizers.l2(0.01)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Activation('tanh'))\n",
        "# model.add(Conv2D(32, kernel_size=(3, 3), padding = 'same', strides = (1,1), kernel_regularizer=regularizers.l2(0.01)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Activation('tanh'))\n",
        "# model.add(Conv2D(32, kernel_size=(3, 3), padding = 'same', strides = (1,1), kernel_regularizer=regularizers.l2(0.01)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Activation('tanh'))\n",
        "# model.add(Conv2D(8, kernel_size=(3, 3), padding = 'same', strides = (1,1), kernel_regularizer=regularizers.l2(0.01)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Activation('tanh'))\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(62, activation='softmax' ))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=tuple(X_train.shape[1:])))\n",
        "model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(BatchNormalization())\n",
        "# model.add(Conv2D(256, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01))) #maybe suppress this layer and instead train for batch size = 64, epochs = 20\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Conv2D(512, (3, 3), activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "# model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.5))\n",
        "model.add(Dense(62, activation='softmax'))\n",
        "\n",
        "epochs = 30\n",
        "batch_size = 64\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])\n",
        "es = EarlyStopping(monitor='val_accuracy', mode='max', min_delta=0.5)\n",
        "model.fit(X_train, Y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=2)\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "886/886 - 9s - loss: 2.5259 - accuracy: 0.7656\n",
            "Epoch 2/30\n",
            "886/886 - 9s - loss: 1.0742 - accuracy: 0.8085\n",
            "Epoch 3/30\n",
            "886/886 - 9s - loss: 0.9418 - accuracy: 0.8201\n",
            "Epoch 4/30\n",
            "886/886 - 9s - loss: 0.8799 - accuracy: 0.8270\n",
            "Epoch 5/30\n",
            "886/886 - 9s - loss: 0.8433 - accuracy: 0.8314\n",
            "Epoch 6/30\n",
            "886/886 - 9s - loss: 0.8035 - accuracy: 0.8372\n",
            "Epoch 7/30\n",
            "886/886 - 9s - loss: 0.7761 - accuracy: 0.8386\n",
            "Epoch 8/30\n",
            "886/886 - 9s - loss: 0.7571 - accuracy: 0.8424\n",
            "Epoch 9/30\n",
            "886/886 - 9s - loss: 0.7195 - accuracy: 0.8462\n",
            "Epoch 10/30\n",
            "886/886 - 9s - loss: 0.6958 - accuracy: 0.8468\n",
            "Epoch 11/30\n",
            "886/886 - 9s - loss: 0.6719 - accuracy: 0.8500\n",
            "Epoch 12/30\n",
            "886/886 - 9s - loss: 0.6418 - accuracy: 0.8530\n",
            "Epoch 13/30\n",
            "886/886 - 9s - loss: 0.6306 - accuracy: 0.8550\n",
            "Epoch 14/30\n",
            "886/886 - 9s - loss: 0.6192 - accuracy: 0.8555\n",
            "Epoch 15/30\n",
            "886/886 - 9s - loss: 0.6045 - accuracy: 0.8550\n",
            "Epoch 16/30\n",
            "886/886 - 9s - loss: 0.6003 - accuracy: 0.8580\n",
            "Epoch 17/30\n",
            "886/886 - 9s - loss: 0.5908 - accuracy: 0.8568\n",
            "Epoch 18/30\n",
            "886/886 - 9s - loss: 0.5832 - accuracy: 0.8598\n",
            "Epoch 19/30\n",
            "886/886 - 9s - loss: 0.5792 - accuracy: 0.8594\n",
            "Epoch 20/30\n",
            "886/886 - 9s - loss: 0.5705 - accuracy: 0.8612\n",
            "Epoch 21/30\n",
            "886/886 - 9s - loss: 0.5641 - accuracy: 0.8611\n",
            "Epoch 22/30\n",
            "886/886 - 9s - loss: 0.5602 - accuracy: 0.8622\n",
            "Epoch 23/30\n",
            "886/886 - 9s - loss: 0.5606 - accuracy: 0.8617\n",
            "Epoch 24/30\n",
            "886/886 - 9s - loss: 0.5535 - accuracy: 0.8631\n",
            "Epoch 25/30\n",
            "886/886 - 9s - loss: 0.5532 - accuracy: 0.8639\n",
            "Epoch 26/30\n",
            "886/886 - 9s - loss: 0.5486 - accuracy: 0.8636\n",
            "Epoch 27/30\n",
            "886/886 - 9s - loss: 0.5420 - accuracy: 0.8649\n",
            "Epoch 28/30\n",
            "886/886 - 9s - loss: 0.5404 - accuracy: 0.8645\n",
            "Epoch 29/30\n",
            "886/886 - 9s - loss: 0.5404 - accuracy: 0.8641\n",
            "Epoch 30/30\n",
            "886/886 - 9s - loss: 0.5400 - accuracy: 0.8656\n",
            "Test loss: 0.6562535762786865\n",
            "Test accuracy: 0.8347564935684204\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPrVqOGz7Y-L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ee9e368-606e-49e1-f90f-96b3252c4c83"
      },
      "source": [
        "top1 = 0.0\n",
        "top5 = 0.0    \n",
        "class_probs = model.predict(X_test)\n",
        "for i, l in enumerate(Y_test):\n",
        "  label = np.where(l == 1)\n",
        "  class_prob = class_probs[i]\n",
        "  top_values = (-class_prob).argsort()[:3]\n",
        "  if top_values[0] in label:\n",
        "    top1 += 1.0\n",
        "  if np.isin(np.array(label), top_values):\n",
        "    top5 += 1.0\n",
        "\n",
        "print(\"top1 acc\", top1/len(Y_test))\n",
        "print(\"top1 acc\", top5/len(Y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "top1 acc 0.8347564832384566\n",
            "top1 acc 0.9716951296647691\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfPWth7TC9S7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeab3c88-f9d1-4e9d-b25f-91f5ede2db38"
      },
      "source": [
        "model.save('classifierpaper_EnglishFNT_0-1.h5')\n",
        "model.count_params()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12231486"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LG54-Adtst_o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b19b611f-6b60-4df2-8dc8-ebc9a8eb654d"
      },
      "source": [
        "model2 = keras.models.load_model('classifierpaper_EnglishFNT.h5')\n",
        "model2.count_params()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "56958"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAMc-upWt3D6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}